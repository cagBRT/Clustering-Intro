{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOJ3EPUq2aQL46CgmodYRq0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/Clustering-Intro/blob/master/Evaluating_the_performance_of_a_clustering_algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating the performance of a clustering algorithm"
      ],
      "metadata": {
        "id": "RGRvDRzKPW5k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating the performance of a clustering algorithm is not as trivial as counting the number of errors or the precision and recall of a supervised classification algorithm."
      ],
      "metadata": {
        "id": "e8RVl3UtMHn7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Any evaluation metric should **not** take the absolute values of the cluster labels into account<br>\n",
        "\n",
        "Instead, if the clustering <br>\n",
        "- defines separations of the data similar to some ground truth set of classes<br>\n",
        "or <br>\n",
        "-satisfying some assumption such that members belong to the same class are more similar than members of different classes according to some similarity metric.\n",
        "\n"
      ],
      "metadata": {
        "id": "n81guoAuMnLb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Rand Index**"
      ],
      "metadata": {
        "id": "EGgNG1sDNIen"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the ground truth class assignments are labels_true<br>\n",
        "and<br>\n",
        "The clustering algorithm assignments of the same samples are labels_pred <br>\n",
        "\n",
        "The (adjusted or unadjusted) **Rand index is a function that measures the similarity of the two assignments, ignoring permutations:**\n",
        "\n"
      ],
      "metadata": {
        "id": "LikQ8IwbNTTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics"
      ],
      "metadata": {
        "id": "l_X-jeFANNgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_true = [0, 0, 0, 1, 1, 1]\n",
        "labels_pred = [0, 0, 1, 1, 2, 2]\n",
        "\n",
        "#two 0,0 and two 1,2 and one 1,1 - agreeing"
      ],
      "metadata": {
        "id": "VdJikPCENNlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aq6FP5WsLXXu"
      },
      "outputs": [],
      "source": [
        "metrics.rand_score(labels_true, labels_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Rand index does not ensure to obtain a value close to 0.0 for a random labelling. The adjusted Rand index corrects for chance and will give such a baseline."
      ],
      "metadata": {
        "id": "ANxdx_DWN_iY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics.adjusted_rand_score(labels_true, labels_pred)"
      ],
      "metadata": {
        "id": "TzjM-E4VLaLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change the predictions<br>\n",
        "As with all clustering metrics, one can permute 0 and 1 in the predicted labels, rename 2 to 3, and **get the same score**"
      ],
      "metadata": {
        "id": "u-67q72XOGSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels_true = [0, 0, 0, 1, 1, 1]\n",
        "labels_pred = [1, 1, 0, 0, 3, 3]\n",
        "metrics.rand_score(labels_true, labels_pred)"
      ],
      "metadata": {
        "id": "ism19UUfLfD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics.adjusted_rand_score(labels_true, labels_pred)"
      ],
      "metadata": {
        "id": "qlQATVKILlKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Swapping the argument does not change the scores, they can be used as consensus measures"
      ],
      "metadata": {
        "id": "jDfWJbqEOgEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics.rand_score(labels_pred, labels_true)"
      ],
      "metadata": {
        "id": "ajunnf0fLxGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics.adjusted_rand_score(labels_pred, labels_true)"
      ],
      "metadata": {
        "id": "zE-LTGJ1Lzfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Perfect labeling is scored 1.0**"
      ],
      "metadata": {
        "id": "5csOiW_OOrEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels_pred = labels_true[:]\n",
        "metrics.rand_score(labels_true, labels_pred)\n"
      ],
      "metadata": {
        "id": "96mCU-ORL3Rx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics.adjusted_rand_score(labels_true, labels_pred)"
      ],
      "metadata": {
        "id": "qoZXo_p5L5SM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Poorly agreeing labels (e.g. independent labelings) have lower scores <br>\n",
        "\n",
        "- **the adjusted Rand index** the score will be negative or close to zero.\n",
        "\n",
        "- for the **unadjusted Rand index** the score, while lower, will not necessarily be close to zero"
      ],
      "metadata": {
        "id": "SzNbDenmOyhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels_true = [0, 0, 0, 0, 0, 0, 1, 1]\n",
        "labels_pred = [0, 1, 2, 3, 4, 5, 5, 6]\n",
        "metrics.rand_score(labels_true, labels_pred)\n",
        "\n",
        "#non-agreeing 0,0...0,1...0,5 and 1,5. disagreeing"
      ],
      "metadata": {
        "id": "7NPJ2kRcL7cu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics.adjusted_rand_score(labels_true, labels_pred)"
      ],
      "metadata": {
        "id": "l537PBFIL9JD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Mutual Information based scores**"
      ],
      "metadata": {
        "id": "ff-TytWGPdWH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mutual Information is a function that measures the agreement of the two assignments, ignoring permutations."
      ],
      "metadata": {
        "id": "Xb_M_4SPQK9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels_true = [0, 0, 0, 1, 1, 1]\n",
        "labels_pred = [0, 0, 1, 1, 2, 2]"
      ],
      "metadata": {
        "id": "ANIWekPbP4et"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics.adjusted_mutual_info_score(labels_true, labels_pred)"
      ],
      "metadata": {
        "id": "5x2XHZGdP8mR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can permute 0 and 1 in the predicted labels, rename 2 to 3 and get the same score:"
      ],
      "metadata": {
        "id": "U-XsKY48QGQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels_true = [0, 0, 0, 1, 1, 1]\n",
        "labels_pred = [1, 1, 0, 0, 3, 3]\n",
        "metrics.adjusted_mutual_info_score(labels_true, labels_pred)"
      ],
      "metadata": {
        "id": "Iv056-rTQEK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Swapping the argument does not change the score. Thus they can be used as a consensus measure:\n",
        "\n"
      ],
      "metadata": {
        "id": "FlgxAvzCQhUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics.adjusted_mutual_info_score(labels_pred, labels_true)"
      ],
      "metadata": {
        "id": "by7L3pIxQk7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perfect labeling is scored 1.0"
      ],
      "metadata": {
        "id": "B9nRBmmnREf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels_pred = labels_true[:]\n",
        "metrics.adjusted_mutual_info_score(labels_true, labels_pred)"
      ],
      "metadata": {
        "id": "Aw9FBzKhQrSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics.normalized_mutual_info_score(labels_true, labels_pred)"
      ],
      "metadata": {
        "id": "vSS4b7RpQtnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bad (e.g. independent labelings) have non-positive scores:"
      ],
      "metadata": {
        "id": "PE056ykJQ4fv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels_true = [0, 1, 2, 0, 3, 4, 5, 1]\n",
        "labels_pred = [1, 1, 0, 0, 2, 2, 2, 2]\n",
        "metrics.adjusted_mutual_info_score(labels_true, labels_pred)"
      ],
      "metadata": {
        "id": "_pzApcofQ1b5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}